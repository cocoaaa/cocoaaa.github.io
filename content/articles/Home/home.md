Title: Welcome! 
Modified: 2021-02-19  
Template: home  
URL:  
save_as: index.html  
memo: home  
javascripts: https://www.amcharts.com/lib/4/core.js, https://www.amcharts.com/lib/4/charts.js, https://www.amcharts.com/lib/4/maps.js, https://www.amcharts.com/lib/4/geodata/worldLow.js, https://www.amcharts.com/lib/4/geodata/usaTerritoriesLow.js, https://www.amcharts.com/lib/4/themes/animated.js, https://www.amcharts.com/lib/4/plugins/timeline.js,  my_trajectory.js
stylesheets: my_trajectory.css

<div id="chartdiv" height="400"></div>
<div id="mapdiv" height="600"></div>

I'm a 3rd year Computer Science PhD at USC studying computational models of cognitive representation using generative models. 
My general research interest lies at the intersection of representation learning and information theory,  inspired by the way human perceptual system integrates multimodal sensory inputs. 
My journey started from noticing our own ability to (i) break down a complex data into abstract, digestable chunks of concepts and (ii) create a new idea by playing and recombining the conceptual building blocks.  For instance, we can take a glimpse of this process of abstraction and synthesis in Picasso's video and  The video of Picasso's abstracting process gives us a glimpse into an example of such process happening: 

<iframe width="1195" height="671" src="https://www.youtube.com/embed/Xwbuw1CSFew" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="1195" height="691" src="https://www.youtube.com/embed/JU9oaD0e7uU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="672" height="480" src="https://www.youtube.com/embed/Nxes8pyHkJc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<iframe width="672" height="480" src="https://www.youtube.com/embed/ylh0X38yvQI?t=411" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


In particular, I'm intrigued by how a semantic content can be expressed in various forms (eg. in languages, in images, in gestures, in sounds, and infinitely many forms within each of the domains), and conversely, how seamlessly we extract a common semantic content from observations in vastly different representational forms.

---



studying the lower-dimensional representation of complex unstructured datasets (eg. multi-spectral satellite images, 3D MRI scans) using deep learning. My work lies in the intersection of statistical machine learning, information theory and coding theory.  I also work on interactive visualization of high-dimensional datasets and complex models (eg. deep neural nets) for a guide to better insight and understanding.

- Information Distillation across domain and modality: Variational Autoencoder
- Invariance in representation: information-theory, "nuisance"
- Dynamics of learning in neural networks: initialization, normalization
    - non-linear and chaos theory
    - emergence
    - information geometry

Before my PhD, I was at MIT studying Mathematics and EECS (Electrical Engineering and Computer Science) for my undergraduate and Masters. After Masters, I interned at Apple as a coop for 9 months.  I also worked on a cool French robotics startup (Keecker) and several academic research labs (MIT CSAIL, MIT Media Lab and INRIA) during my studies.


<!--![my-trajectory](/images/my-trajectory.png)-->
More about [me](/pages/about-me.html) and papers [here](/pages/publications.html).

---
More on my research interest 

I'm interested in distilling the informational content from the representational structure imposed by domain-specific structures (eg. natural  images satisfy and define a set of  rules that make us('intelligent' entities) to recognize them as 'natural' images,  whereas there is an implicit  set of rules that defines what a proper 'English' sentence (as opposed to 'French' language or a video data).   also work on understanding the dynamics of neural networks from information-theoretic perspectives. 



## Recent updates
- I gave my first tutorial @ PyData LA, 2019 on "Experimental ML with Holoviews/Geoviews + Pyorch". Here are my talk [slides](/pdfs/experimental-ml-2019-hayley.pdf), [video](#), and [jupyter notebook materials](https://github.com/cocoaaa/PyData-LA-2019)!

| | | |
|---|---|---|
|![pydata-0](/images/pydata-0.png)|![pydata-1](/images/pydata-1.png)|![pydata-1-2](/images/pydata-1-2.png) |
|![pydata-2](/images/pydata-2.png)|![pydata-3](/images/pydata-3.png)| ![pydata-4](/images/pydata-4.png)|

- I participated in [Geo4Good](https://sites.google.com/earthoutreach.org/geoforgood19/home) @ Google in Mtn View, CA! Check out some [highlights](https://tinyurl.com/wdoyepy) of inspiring project going on using Google Earth Engine and Studio. 
- New post: "Total variation, KL-Divergence, Maximum Likelihood"
- New post: "Let's be honest: peeling the assumptions that get us to Variational Autoencoders"
- New post: "Thinking about an observer vs. the observed"

## What am I upto?
- Actively looking for a fun, challenging, meaningful internship
- Working on a cool paper using variational autoencoder on geospatial data

---
#### TMI ('Too Much Information')

More importantly, I'm practicing to:
- observe without being entangled emotionally and(?) physically
- look at small thoughts carefully
- not to rush
- spend most of time on what matters most
- be gentle and be slow
- be curious
- question
- continuously bring stochasity to the current model of world as I believe and,
- hopefully replace my old thinking and habits with more conscious choices and freer future

