<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Small Simplicity</title>
    <link rel="shortcut icon" type="image/png" href="https://cocoaaa.github.io/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="https://cocoaaa.github.io/favicon.ico">
    <link href="https://cocoaaa.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Small Simplicity Full Atom Feed" />
    <link href="https://cocoaaa.github.io/feeds/home.atom.xml" type="application/atom+xml" rel="alternate" title="Small Simplicity Categories Atom Feed" />

    <!--[if lte IE 8]>
        <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-old-ie-min.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
         <link rel="stylesheet" href="https://unpkg.com/purecss@1.0.1/build/grids-responsive-min.css">
    <!--<![endif]-->

    <link href="https://fonts.googleapis.com/css?family=Fira+Code:wght@500|Fira+Sans+Condensed|Cantarell|VT323&display=swap" rel="stylesheet">


    <link rel="stylesheet" href="https://cocoaaa.github.io/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="https://cocoaaa.github.io/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="https://cocoaaa.github.io/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
    <meta name="description" content="" />
    <meta name="author" content="Hayley Song" />
</head>

<body>
    <header>
        <!-- here: add menu items for navbar -->
        <nav> 
            <ul>
                <li class="selected"><a href="https://cocoaaa.github.io/">Home</a></li>
                <li><a href="https://cocoaaa.github.io/pages/about-me">About</a></li>
                <li><a href="https://cocoaaa.github.io/pages/publications">Publications</a></li>
                <li><a href="https://cocoaaa.github.io/pages/projects">Projects</a></li>
                <li><a href="https://cocoaaa.github.io/blog_index">Blog</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="https://cocoaaa.github.io/">Small Simplicity</a></h1>
            <h2>Understanding Complex Intelligent Systems from Computational Perspective</h2>
        </div>
    </header>

<!-- here: main content section's css -->
    <div id="wrapper" class="pure-g">

        <div id="content pure-u-1 pure-u-md-4-5">
    <div class="page">
        <h1>Welcome!</h1>
        <!-- >> testing playground -->
<!-- <div class=sidebar>
testing sidebar
- try also to add an image (eg. my profile)
</div> -->
<!-- add my profile picture to the right side -->
<!-- <div class=marginnote>
<figure>
    <img src="https://firebasestorage.googleapis.com/v0/b/firescript-577a2.appspot.com/o/imgs%2Fapp%2Fcocoaaa%2FOzdNmX2UzQ.png?alt=media&token=b246a2e5-5e52-418d-8c96-afbbbe84d1b7" alt="multimodal-inputs" style="width:60%">
</figure> -->
<!-- Add my recent update section to the right side -->
<!-- <h1 align="center"> Right margin: Recent updates! </h1>

</div> -->
<!-- add my profile picture to the left side margin -->
<div align="center" class="marginnoteleft">
<!-- add my profile picture to the right side -->
<figure>
<img alt="profile" src="/images/profile.jpg" width="250"/>
</figure>
<!-- my name and address -->
<div>
<h1 id="hae-jin-hayley-song">Hae Jin (Hayley) Song</h1>
<h4 id="fellow-at-berkman-klein-center-at-harvard-university">Fellow at Berkman Klein Center at Harvard University</h4>
<h4 id="ai-research-fellow-at-thoughtworks-inc">AI Research Fellow at ThoughtWorks Inc.</h4>
<h4 id="phd-in-computer-science">Ph.D. in Computer Science</h4>
<!--
    USC Viterbi School of Engineering,
    <br/>

    Information Sciences Institute (ISI)
    <br/>
-->
<!-- 4676 Admiralty Way, Suite 1001 -->
<!-- <br/> -->
<!-- Marina Del Rey, CA 90292 -->
<!-- <br/> -->
<!-- email: haejinso@usc.edu  -->
</div> <!-- end name, address -->
<!-- display buttons for cv, git, pubs, projects  -->
<div align="center">
<!-- <div class="image-text-container"> -->
<a href="/docs/hjsong_cv.pdf">| CV | <img alt="cv" src="/images/cv1.png" width="40"/></a>
<!-- </div> -->
<!-- <div class="image-text-container"> -->
<a href="https://github.com/cocoaaa">Github | <img alt="@cocoaaa" src="/images/github.svg" width="40"/></a>
<!-- </div> -->
<!-- <div class="image-text-container"> -->
<a href="/pages/publications.html">Pubs | <img alt="publications" src="/images/publication.png" width="40"/></a>
<!-- </div> -->
<!-- <div class="image-text-container"> -->
<a href="/pages/projects.html">Projects | <img alt="projects" src="/images/lightbulb3.svg" width="40"/></a>
<!-- </div> -->
<!-- <div class="image-text-container"> -->
<!-- <a href="/docs/hjsong_rs.pdf">Research |<img src="/images/lightbulb3.svg" alt="research-statement" width="40"/></a> -->
<!-- </div> -->
</div>
<!-- todo: temp: link to my rs (invisible, but only to keep the link available by directly going to the link(ie. cocoaaa.github.io/docs/hjsong_rs.pdf)) -->
<a href="/docs/hjsong_rs.pdf"></a>
</div>
<!-- Add my recent update section to the right side -->
<!-- end sidenote for profile  --><div class="marginnote">
<!-- <div class=marginnote align="center"> -->
<h1 align="center" id="recent-updates_1"> Recent updates! </h1>
<!-- todo: add cvpr, lg meet spotlight, neurips, and the rest from what i already had in the blog -->
<div ;="" style="font-size:0.9em">
<ul>
<!-- <li></li> -->
<li>2026 Jan: Working on updating my blog...:-) </li>
<li>2025 Nov: Attending <a href="https://www.mis.mpg.de/events/series/workshop-on-geometry-topology-and-machine-learning-gtml-2025">Workshop on Geometry, Topology, and Machine Learning (GTML 2025)</a>  at  Max Planck Institute for Mathematics in the Sciences. Excited to meet new friends working at the intersection of geometry and machine learning! </li>
<li>2025 Oct: Very grateful and excited for this opportunity to join <strong><a href="https://cyber.harvard.edu/">Berkman Klein Center at Harvard University</a></strong> as AI fellow, working on principled methods of scalable and robust AI design, development and governance.
<li>2025 June: Excited to share that our paper on "Riemannian-geometric fingerprints of generative models" is accepted to <strong>ICCV 2025</strong> as a highlight paper! Please check it out here: <a href="https://arxiv.org/abs/2506.22802">https://arxiv.org/abs/2506.22802</a></li>
<li>2024 Feb: Our paper (ManiFPT: On Defining and Analyzing the Fingerprints of Generative Models) is accepted to <strong>CVPR 2024</strong>!</li>
<li>2024 May: I am invited to an AI meeting hosted by LG AI Research in the Bay Area to give a spotlight talk on my research. Thank you!</li>
<li>2023 Dec: Our paper on model attribution is accepted to <strong>NeurIPS Workshop</strong> on Attribution at Large Scale.</li>
<li>New post: "Total variation, KL-Divergence, Maximum Likelihood"</li>
<li>New post: "Let's be honest: peeling the assumptions that get us to Variational Autoencoders"</li>
<li>New post: "Thinking about an observer vs. the observed"</li>
<li>I am preparing for a talk at <a href="https://www.scipy2021.scipy.org">SciPy 2021</a> in July.</li>
<li>I got accepted to <a href="https://www.santafe.edu/engage/learn/schools/sfi-complex-systems-summer-school">2021 Complex Systems Summer School</a> at Santa Fe Institute, Woohoo!</li>
<li>I gave my first tutorial @ PyData LA, 2019 on "Experimental ML with Holoviews/Geoviews + Pytorch". Here are my talk <a href="/pdfs/experimental-ml-2019-hayley.pdf">slides</a>, <a href="#">video</a> and <a href="(https://github.com/cocoaaa/PyData-LA-2019">jupyter notebook materials</a>!</li>
<li>I participated in <a href="https://sites.google.com/earthoutreach.org/geoforgood19/home">Geo4Good</a> @ Google in Mtn View, CA! Check out some <a href="https://tinyurl.com/wdoyepy">highlights</a> of inspiring projects based on Google Earth Engine and Studio. </li>
</li></ul>
</div>
</div>
<!-- end of testing playground -->
<!-- ver: my bkc profile: updated here on 20260115-012143-->
<!--todo: resume here -->
<p>I am currently a Fellow at the <a href="https://cyber.harvard.edu/">Berkman Klein Center at Harvard University</a> and an AI Research Fellow at ThoughtWorks, where I work on the geometric foundations of AI interpretability and safety. </p>
<p>My research develops formal, geometry-based frameworks for understanding, analyzing, and steering the behavior of modern generative models. 
I hold a Ph.D. in Computer Science from the University of Southern California. I earned my B.S. and M.Eng. in Electrical Engineering and Computer Science from MIT, with a minor in Mathematics. 
During her M.Eng., I specialized in artificial intelligence and worked under the joint guidance of Prof. Regina Barzilay and Dr. Julian Straub on computer vision problems in medical imaging, including non-rigid mammogram registration for breast cancer detection and 3D reconstruction of human arms for lymphedema screening. </p>
<p>Across my academic career, I have conducted research at MIT (CSAIL, Media Lab, McGovern Institute), INRIA (ILDA Lab), and USC (Information Sciences Institute, Visual Intelligence and Multimedia Analytics Laboratory (VIMAL), iLab, Knowledge Computing Lab). I have also had industry research internships at Apple, MathWorks, and a French robotics startup, Keecker. </p>
<h2 id="research-interests">Research Interests</h2>
<h3 id="geometric-foundation-of-generative-models">Geometric Foundation of Generative Models</h3>
<h3 id="applications-for-scalable-and-robust-ai-design-development-and-governance">&amp; Applications for Scalable and Robust AI Design, Development and Governance</h3>
<p>My current research focuses on how complex, high-dimensional information-processing systems, particularly modern generative models and other AI models, behave, using tools from differential geometry, Riemannian manifolds, latent-space topology and Bayesian methods.</p>
<p>My goal is to develop principled and scalable methods to characterize, attribute, and control model behaviors by identifying geometric &ldquo;fingerprints&rdquo; of their internal representations and dynamics. My work addresses foundational questions in the analysis and controllability of generative models, with applications to AI interpretability, model attribution, deepfake detection, bias and degeneration analysis, and model steering for safety and alignment. </p>
<p>My recent work has made significant contributions to these questions through research on fingerprinting generative models: In papers published at CVPR 2024 and ICCV 2025 (Highlight), we introduced a novel theoretical framework that represents model behavior on data manifolds and formalized the notions of &ldquo;artifacts&rdquo; and &ldquo;fingerprints&rdquo; of generative models in a geometric language. This framework enables effective attribution and systematic comparison of state-of-the-art generative models. </p>
<p>More broadly, through this line of research, I aim to advance a generalized theory of generative models and their internal mechanisms, to help shape the safe and responsible integration of Generative AI into our society in ways that serve humanity.</p>
<!-- ## Research Interests -->
<h3 id="geometric-fingerprints-of-generative-models">Geometric Fingerprints of Generative Models</h3>
<p>Broadly, my research interest lies in understanding how complex, high-dimensional information-processing systems (e.g., human intelligence, modern generative models, collective behaviors like traffic patterns) behave and developing efficient algorithms to analyze their characteristics and traces in a principled way, from geometric perspectives. 
Through this geometric understanding of their properties and internal mechanisms, I aim to improve their behaviors (e.g., steering away from degenerative patterns and biases while aligning to more proper values), by mechanistically intervening in their internal causal pathways. Of many such complex systems, my PhD research focuses on generative models and representation learning algorithms.
 <!-- that are responsible for different behavioral characteristics  --></p>
<!-- (e.g., defects in Vision Generative Models, hallucinations in LLMs).  -->
<!--  opt1-->
<!-- current gap -->
<p>Currently, despite the rapid development of Generative A.I., there is still a big gap in our understanding and control over their designs and behaviors.
<span class="blue bold">My aim</span> is to fill this gap, by developing a formal framework and efficient algorithms that can <strong>represent</strong>, <strong>analyze</strong> and <strong>control</strong> the <strong>behaviors of generative models</strong> (e.g., Large Foundation Models (FMs)) in their high-dimensional spaces.</p>
<p>To this end, I approach these problems from geometric perspectives, and I am working on formalizing the theory of (generative) model behaviors using <strong>Differential Geometry and Riemannian Manifold</strong>.
Grounded on this theory, I also work to develop efficient algorithms that can extract <strong>geometric signatures</strong> of the behaviors and internal representations of large generative models (which live on much higher dimensional spaces than what the current geometric approach has been employed (e.g., 1D signals or 3D images) ), 
in order to "fingerprint" and characterize them. </p>
<!-- end opt1

<!-- opt2 -->
<!-- I am particularly interested in formalizing the theory of generative models -- in particular, their **high-dimensional behaviors**, characteristics (i.e., behavioral **"fingerprints"**) and **controllability** -- from geometric perspectives. 

To this end, I approach my problems by standing on the rich theory of **Differential Geometry and Riemannian Manifold**, and aim to apply theoretical and computational tools from Riemannian geometry to study generative models: their geometric signatures and controllability on their internal mechanisms.  -->
<!-- end opt2 -->
<figure>
<img alt="multimodal-inputs" src="/images/my-research-statement/gm-tracing-detectives-ver2.png" style="width:80%"/>
</figure>
<!-- These geometric signatures will help address many open and arising problems in generative modeling, by providing principled ways to extract and represent the characteristics of different model behaviors (e.g., as vectors) and quantify their differences. -->
<!-- compare different model behaviors and quantify their characteristics. -->
<!-- in numerics, in a vector space. -->
<!--  as formal measures  -->
<!-- For example, using them we can differentiate amongst different Foundation Models, addressing questions like "is this image generated by DALL-E or IMAGEN?" or "is this document from ChatGPT, LLaMA, or human?".
We can also study what makes one model different from another based on their signatures, and investigate their underlying internal mechanisms that cause specific signatures like artifacts in model-generated images or hallucinations in LLMs.  -->
<!-- I also aim to use these geometric signatures to study and control the internal mechanisms of generative models,  -->
<!-- ## My current project on fingerprinting generative models -->
<h2 id="my-recent-work-and-extensions_1">My recent work and extensions</h2>
<p>I have started on this research endeavor through my work on fingerprinting generative models:
In our recent work published in <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Song_ManiFPT_Defining_and_Analyzing_Fingerprints_of_Generative_Models_CVPR_2024_paper.html">CVPR 24</a>, we proposed a useful theoretical framework to represent model behaviors on a data manifold (i.e., as vector fields on a data manifold constructed from real images).
We also formalized, for
the first time in the literature, the definition of &ldquo;artifacts&rdquo; and &ldquo;fingerprints&rdquo; of generative models in a geometric language, and proposed an effective attribution method to study and compare vision generative models.
(See more details <a href="/pages/publications.html">here</a>.)</p>
<!--  we formalized, for
the first time in the literature, the definition of “artifacts” and “fingerprints” of generative models based on their
data manifolds, and proposed an effective attribution method to study and compare vision generative models -->
<ul>
<li>
<p>I am excited to extend this work to a larger variety of <strong>Foundation Models</strong>, including LLMs and SoTA multimodal models, to study their fingerprints and proactively <strong>watermark</strong> the models. </p>
</li>
<li>
<p>I will also look into their <strong>internal mechanisms</strong> that cause these fingerprints and develop principled methods to  <strong>intervene</strong> and <strong>control</strong> causal pathways that are responsible for different model behaviors (e.g., image artifacts by Vision GMs or hallucinations in LLMs).</p>
</li>
</ul>
<!-- the geometric signatures of generative models, including Foundation Models (FMs) like LLMs and multimodal FMs,  -->
<p>I hope by researching these problems, I will contribute to a generalized theory of generative models and their internal mechanisms, which in turn help shape a  safe and responsible integration of Generative A.I. into our society. </p>
<!-- ---

My aim is to develop a formal framework that 
represent the **complex behaviors of generative models** (e.g., Large Foundation Models) in their high-dimensional spaces, 
and efficient algorithms that help analyze their geometric signatures, in order to characterize and differentiate them in a principled way.

To this end, I am working on formalizing the theory of generative models from geometric perspectives -- using the theory of **differential geometry** -- in particular, with respect to their **high-dimensional behaviors**, **characteristics** (i.e., behavioral "fingerprints") and **controllability**.

The 
My approach is grounded on the rich theory of differential geometry and Riemannian Manifold, and I aim to apply theoretical and computational tools from Riemannian geometry to study generative models.


 -->
<p><br/>
<br/>
<br/></p>
<hr/>
<h4 id="please-see-more-details-on-my-research-in-my-publications-and-projects">Please see more details on my research in my <a href="/pages/publications.html">publications</a> and <a href="/pages/projects.html">projects</a> :)</h4>
<hr/>
<h2 id="archived-details_1">Archived Details</h2>
<details>
<summary> Click to expand </summary>
My journey started from noticing our own ability to (i) break down a complex observation into multiple chunks of smaller and abstract concepts and (ii) create a new idea by playing and recombining the conceptual building blocks.  For instance, we can take a glimpse of this dance between abstraction and synthesis in a video of Picasso's live drawing:

<br/>
<div align="center">
<iframe "="" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="" frameborder="0" height="480" src="https://www.youtube.com/embed/JU9oaD0e7uU" width="672"></iframe>
</div>
<!-- <iframe  height="420" align="left" src="https://www.youtube.com/embed/Xwbuw1CSFew" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<!-- <iframe width="672" height="480" src="https://www.youtube.com/embed/Nxes8pyHkJc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
<!-- <iframe width="672" height="480" src="https://www.youtube.com/embed/ylh0X38yvQI?t=411" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->


More specifically, I'm intrigued by how seamlessly we extract a common semantic content from observations in vastly different representational forms (such as languages, images, gestures or sounds, and infinitely many forms within each modality), and reversely, how a semantic content can be expressed in various forms without losing its (overall) meaning
<label class="margin-toggle sidenote-number" for="sn-note"></label>
<input class="margin-toggle" id="sn-note" type="checkbox"/>
<span class="sidenote">
Hmm.. coarse-graining?</span>.

My exploration starts with an hypothesis that a phenomena in reality, from which
our observations stem from, contains __semantic potentials__("potential" as in
_potential energy_ in Physics, or going further up the stream, as in Aristotle's
["Potentiality and actuality"](https://en.wikipedia.org/wiki/Potentiality_and_actuality)
<label class="margin-toggle sidenote-number" for="sn-note"></label>
<input class="margin-toggle" id="sn-note" type="checkbox"/>
<span class="sidenote">
This idea influenced Leibniz to develop the science of "dynamics". 
Learning about such influence brings light into what Leibniz was struggling to hit the chord with ideas like 'power' and 'action'. Contemplate: Aristotle's "potential:actuality" vs. Leibniz's "power:action"</span>
.) 

I wonder,

- What is the relationship between _semantic information_<label class="margin-toggle sidenote-number" for="sn-note"></label>
<input class="margin-toggle" id="sn-note" type="checkbox"/>
<span class="sidenote">
See [An Outline of a Theory of Semantic Information](https://dspace.mit.edu/bitstream/handle/1721.1/4821/RLE-TR-247-03150899.pdf?sequence=1), a [survey](https://plato.stanford.edu/entries/information-semantic/), and more recent work by [Kolchinsky and Wolpert](https://royalsocietypublishing.org/doi/10.1098/rsfs.2018.0041)
</span>
and _semantic potential_, i.e. the underlying _field_ from which individual observations are actualized into an instance of a natural phenomena, an event?
- What is the process -- or geometric constraints -- that leads the same semantics to different representational forms? Can we learn this process from multimodal data?
- What is the process through which an observer builds an understanding -- an internal representation -- of an event?
    - What is the process through which we identify, extract and encode the invariant semantics from observations in diverse modalities?
    - How can we define and measure the semantic information in our representations, efficiently?
- Can we model these processes by __learning a generative model from data collected from multiple modalities__?

---
## Specific Example
For instance, consider the following observations: $X^A$ is an image of a dog barking on the door, $X^B$ is a recording of a dog barking, and $X^C$ is a sentence written in the English language. The semantic content shared among the observations is "there is a dog barking", and each observation is the result of expressing (synm: rendering, stylizing) the semantic content into a form proper for its modality (ie. image, sound, written English language, respectively). 

My question, at the representational level is, how do we identify the underlying, shared semantic contents from the information about domain-specific variations? 
![multi-modal-encoding-of-semantics](/images/semantic_potential/encoding-semantics-from-origin.png)

- How do we _identify_ what is the type of information that is invariant among observations from multiple domains?
- What discovery process goes into separating the shared contents (invariance across domain) from the domain-specifics?  
- Can we use learning-based approaches to build a computational model of such process (i)more efficiently, (ii)by leveraging large amounts of data available?

Now let's flip the question and consider the process of synthesis. I start with a concept that I'd like to express and communicate. For example, I want to actualize the idea of "a dog barking at the door". If I ask you to express this content as an image, sounds, and an English sentence, what would be the process of such domain-specific actualization of a semantic information?
![generating-semantics-in-multi-domains](/images/semantic_potential/generating-semantics-in-multiple-domains.png)

- What is the process of recombining the encoded representations to make better decisions, derive new conclusions? In particular, what is the underlying structure that defines each modality?

Geometry of a modality space: imposes geometric constraints that an instance must satisfy to be a valid observations of that modality
![geometry-of-modality-space](/images/semantic_potential/geometry-of-modality-space.png)

- E.g. an observation in an image form must satisfy a different set of geometric constraints than that in an acoustic form.
- Can we learn a model of such geometric rules via a generative model with neural networks?

The breakdown of main components of my questions looks as follows:

- Semantic information: Address a limitation of Shannon's theory of Information<label class="margin-toggle sidenote-number" for="sn-note"></label>
<input class="margin-toggle" id="sn-note" type="checkbox"/>
<span class="sidenote">
The symmetry axiom of Shannon's entropy preserves the syntactic meaning in symbols, yet disregards their identities. See [ITTP2018](http://tuvalu.santafe.edu/~simon/it.pdf)</span>.
- Semantic potentials == a natural phenomena -- is this what "nature" is defined as?
- The process of actualizing semantic potentials/information to different modalities
- The process during which an observer builds an understanding of the actualized data point
- Geometry of modality space: what is the underlying geometry that defines an observation as a valid image vs. a valid human voice vs. a valid text? 



---
## Research Statement
### Learning a generative model of multimodal representation
In pursuit of this computational model of understanding and generating multimodal data, I am developing generative models with disentangled representation to jointly learn the analysis and synthesis processes of complex, high-dimensional data (eg. satellite images, knowledge bases) with compact and &ldquo;meaningful&rdquo; representations.&nbsp; 
I'm working with Prof. Wael Abd-Almageed at ISI's [VIMAL](https://vimal.isi.edu/), 
focusing on various types of generative models for this goal.
My project with  Prof. [Yao-Yi Chiang](https://spatial.usc.edu/team-view/yao-yi-chiang/) and Prof. [Craig Knoblock](https://usc-isi-i2.github.io/knoblock/) tackles this line of questions using geospatial data, and aims to learn spatial semantics from data that are collected from diverse sources (eg. satellites, Google Street Map, historical maps) and stored in diverse format (eg. images, graphs). This work has potential applications such as global-scale urban environment analysis, automated map synthesis and systems for monitoring environmental changes.

<!--- Read more about our maptile dataset [here](#).-->

Within the domain of representation learning, I&rsquo;m most interested in variational inference methods, especially recent developments in deep generative models such as variational autoencoders (VAEs) and the idea of adversarial training. 

Using a VAE-variant model and adversarial training, I&rsquo;m investigating how we can build a model that extracts invariance in a dataset of heterogeneous representations via VAEs and adversarial training.&nbsp; One of my current projects investigates this question in the domain of spatial informatics, using our new dataset of map tiles from diverse sources.

<!-- - Read more about my work, "Learning Bipartitioned Representation of ..." -- [In Preparation]. -->

### Next itches
<details>
<summary>More about next steps...</summary>
<ul>
<li>Understanding adversary at the latent space from the perspectives of information flow and non-equilibrium achieved by the adversary, ie. the Maxwell's Demon at the gate that distinguishes the two latent partitions</li>
<ul>
<li>GAN models are often described in the framework of min-max games between a generator and an adversary. In particular, there has been works making a connection between Nash Equilibrium and local minimum of the GAN's objective function. This connection motivates me to view my adversary (at the latent partitions) as an 'information sorter', like the <a href="https://en.wikipedia.org/wiki/Maxwell%27s_demon">Maxwell's Demon</a>. The goal of this information sorter is to organize the semantic information into one latent partition, and the domain-specific information into the other latent partition, so that each partition (equivalent to a gas chamber in Maxwell's thought experiment) contains only its type of information.  This approach will allow me to bring in computational tools from information theory and theromodynamics (flow of information) to understand how the adversarial information sorter actually achieves the partitioned latent space. </li>
</ul>
<li>Evaluation of the disentangled partition requires a measure of semantic information </li>
<ul>
<li>In order to evaluate how well our semantic latent space captures the semantic information in the inputs, we first need a well grounded _definition of the semantic information_, as well as _computational methods to efficiently compute_ the value.</li>
<li>See <a href="https://dspace.mit.edu/bitstream/handle/1721.1/4821/RLE-TR-247-03150899.pdf?sequence=1">An Outline of a Theory of Semantic Information</a>, a nice <a href="https://plato.stanford.edu/entries/information-semantic/">survey</a>, and more recent work by <a href="https://royalsocietypublishing.org/doi/10.1098/rsfs.2018.0041">Kolchinsky and Wolpert</a>.</li>
</ul>
<li>Linking the discovered latent factors to external knowledge graph</li>
<!--     - How can we map the learned latent factors to concepts in a more global knowledge graph (eg. Wikipedia)?-->
</ul></details>



---

## TMI: How much is Too Much Information?
More importantly, I'm practicing to:  
- observe without being entangled in what is personal
- look at small thoughts carefully
- not to rush
- spend most of time on what matters most
- be gentle 
- be slow
- be curious
- question
- relax in discomforts
- greet what is as what is
<!-- - nothing more, nothing less -->
- stay open

<!--## Quotes to sit with-->
&gt; &lsquo;Your act was unwise,&rsquo; I exclaimed, &lsquo;as you see by the outcome.&rsquo;   
He solemnly eyed me.  
&lsquo;When choosing the course of my action,&rsquo; said he,
&lsquo;I had not the outcome to guide me.&rsquo;   
- Ambrose Bierce

&gt; Intention and attention.


<!-- 
> If you understand what you're doing, you're not learning anything.   
- Einstein
 Let questions guide your journey, free of motif or resting on an answer.
- continuously bring stochasity to the current model of world as I believe and,
I worry that too much hopping from the domain of AI/ML to physics, and then ultimately to philosophy will worry my PhD advisors. When the worry quiets itself, however, I see that beaconings of uncomfortable inklings/curious itches often point me to exactly what I should be doing at any moment. 

---
studying the lower-dimensional representation of complex unstructured datasets (eg. multi-spectral satellite images, 3D MRI scans) using deep learning. My work lies in the intersection of statistical machine learning, information theory and coding theory.  I also work on interactive visualization of high-dimensional datasets and complex models (eg. deep neural nets) for a guide to better insight and understanding.

- Information Distillation across domain and modality: Variational Autoencoder
- Invariance in representation: information-theory, "nuisance"
- Dynamics of learning in neural networks: initialization, normalization
    - non-linear and chaos theory
    - emergence
    - information geometry

Before my PhD, I was at MIT studying Mathematics and EECS (Electrical Engineering and Computer Science) for my undergraduate and Masters. After Masters, I interned at Apple as a coop for 9 months.  I also worked on a cool French robotics startup (Keecker) and several academic research labs (MIT CSAIL, MIT Media Lab and INRIA) during my studies.

More about [me](/pages/about-me.html) and papers [here](/pages/publications.html).
---
version1
I'm a 3rd year Computer Science PhD at USC working on a computational model of multimodal representation using generative models.  
My research interest lies at the intersection of representation learning and information theory, inspired by the way our perceptual system integrates multimodal sensory inputs via identifying invariant semantics.  

Before my PhD, I was at MIT studying Mathematics and EECS (Electrical Engineering and Computer Science) for my undergraduate and Masters.  During my studies, I interned at a French robotics startup (Keecker) and academic research labs in MIT CSAIL, MIT Media Lab and INRIA.  After Masters, I worked at Apple as a COOP for 9 months.

My journey started from noticing our own ability to (i) break down a complex observation into multiple chunks of smaller and abstract concepts and (ii) create a new idea by playing and recombining the conceptual building blocks.  For instance, we can take a glimpse of this process of abstraction and synthesis in this video of Picasso's live drawing.


--></details>
    </div>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="https://cocoaaa.github.io/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
</body>


</html>