Title: About me

<div align="center"> 
    <img src="/images/profile.jpg" alt="profile" width="250"/>
</div>
<br>
<div align="center">
    <a href="/docs/hjsong_cv.pdf"><img src="/images/cv1.png" alt="cv" width="40"/></a>
    <a href="https://github.com/cocoaaa"><img src="/images/github.svg" alt="@cocoaaa" width="40"/></a>
    <a href="/pages/publications.html"><img src="/images/lightbulb3.svg" alt="projects" width="40"/></a>
</div>

Hello! I'm Hayley, and I'm a Computer Science PhD student at **University of Southern California** (USC). 
I am excited about problems regarding human perception of the world and how symbolic representation of knowledge can facilitate learning in a new domain via information transfer and generalization across domains and modalities.

My research lies at the intersection of **representation learning** and **information** theory, inspired by the way our perceptual system integrates **multimodal** sensory inputs via identifying **invariant semantics**. 

My current guiding question is,
> How do we, an intelligent agent, understand observations from multiple modalities (e.g. images, audio signals and written texts), and 

> How do we extract and build representations of the semantics that is invariant across the multimodal observations?

I am developing generative models to jointly learn the analysis and synthesis processes of multimodal data.  My most recent work introduces a **generative model with disentangled representation** that learns spatial semantics from map tiles collected from diverse sources such as satellites, Google Street Map and custom rendering engines.
I am also interested in understanding how the **semantic information** flows while processing observations from multiple modalities, using tools in deep learning and thermodynamic approaches to information flow.


## Research Homes
At USC, I'm working with Prof. Wael Abd-Almageed at ISI's [VIMAL](https://vimal.isi.edu/). I previously worked with Prof. [Craig Knoblock](http://usc-isi-i2.github.io/knoblock/) 
at ISI's [Center on Knowledge Graphs](http://usc-isi-i2.github.io/home/) and
Prof. [Yao-Yi Chiang](https://yaoyichi.github.io/) at the [Spatial Computing and Informatics](http://spatial-computing.github.io/) lab.

Before USC, I studied Mathematics and Electrical Engineering and Computer Science (EECS) for my Bachelors and Masters at **Massachusetts Institute of Technology**.
During my Masters, I concentrated on Artificial Intelligence and worked under the joint guidance of Professor [Regina Barzilay](https://people.csail.mit.edu/regina/), Professor [Wojciech Matusik](http://people.csail.mit.edu/wojciech/), and Dr. [Julian Straub](http://people.csail.mit.edu/jstraub/). 
My main projects were (1) image registration of mammograms for breast cancer detection, and (2) 3D reconstruction of human arms for efficient [lymphedema](https://mayocl.in/2S5khTZ) screening.  You can find out more about them [here](/pages/publications.html).


<!--
- Build a camera system using 8 RGBD sensors (eg. Intel RealSense)
- Reconstruct 3D models of human arms from RGBD images
- Non-rigid registration of mammogram images using optical flow algorithms
-->

Please see the [project](/pages/publications.html) page for more details on my work.

## Lingering questions
<details>
<summary> Click to expand </summary>

In a bigger scheme, I am excited about the problems regarding human perception of 
the world and how symbolic representation of knowledge can facilitate learning 
in a new domain <em>via knowledge transfer across various domains/modalities</em>.  I'm continuously exploring these questions in my research:

<ul>
<li> How can intelligent agents learn with less supervision, particularly in the domain of vision and three-dimensional perception (<todo>:spatial reasonging?)</li>
  <ul>
  <li>via autonomously interacting with the environment</li>
  <li>via incorporating external knowledge</li>
  <li>via incorporating common sense reasoning</li>
  </ul>
   My [project](#semantic_road_project) on road detection from satellite images explores this question using external geospatial knowledge base (OpenStreetMap) and transfer learning. 

<li>How can those knowledge be represented in a more abstract form so that it can be used for learning in different domains</li>
    <ul>
    <li>Knowledge Representation, Transfer Learning, Domain Adaptation</li>
    </ul>

</ul>


</details>

## Softwares
I use Python (eg. **PyTorch**, Numpy, scikit-learn) for machine learning
[projects](#) and C++ for hardware systems (eg. Microsoft Kinnect and Intel RealSense) as in [this](#) project. 


---
## 


Besides working on my projects, I enjoy being in nature and trying out different sports. 
Along the way, I became a certified scuba diver and have sky-dived in Czech sky! 
I enjoy biking and swimming -- they help me connect to the dimension that 
is not about thinking and analyzing, and remind me we are more than our 
thoughts and minds. I enjoy sharing such experiences with friends:)
 
I occasionally write [here](/blog_index) to process and share what I learn. I would love to hear from you if you have any questions or thoughts:)

## Contact 
Happy to meet new friends and share ideas! Feel free to contact me at:




<div style="float:left;">
    <img src="/images/about-moi/usc-courriel.png" alt="courriel" height="70px" width="140px" style="vertical-align:middle" />
</div>
<div style="float:left;">
    <img src="/images/about-moi/courriel-domain.png" alt="domain" height="70px" width="140px" style="vertical-align:middle" />
</div>
 
<!--
{% img ../images/profile.jpg %}  
<div align="center"> 
    <img src="/images/profile.jpg" alt="profile" width="250"/>
    <ul>
      <li><a href="/docs/hjsong_cv.pdf"><img src="/images/cv1.svg" alt="cv" width="50"/><a></li>
      <li><a href="https://github.com/cocoaaa"><img src="/images/github.svg" alt="@cocoaaa" width="50" /><a></li>
      <li><a href="/pages/projects.html"><img src="/images/lightbulb3.svg" alt="projects" width="50"/><a></li>
    </ul>
</div>



    <a href="/docs/hjsong_cv.pdf"><img src="/images/cv1.svg" alt="cv" width="50"/><a>
    <a href="https://github.com/cocoaaa"><img src="/images/github.svg" alt="@cocoaaa" width="50" /><a>
    <a href="/pages/projects.html"><img src="/images/rocket1.svg" alt="projects" width="50"/><a>
    <a href="/pages/projects.html"><img src="/images/rocket2.svg" alt="projects" width="50"/><a>
    <a href="/pages/projects.html"><img src="/images/lightbulb1.svg" alt="projects" width="50"/><a>
    <a href="/pages/projects.html"><img src="/images/lightbulb2.svg" alt="projects" width="50"/><a>
    <a href="/pages/projects.html"><img src="/images/lightbulb3.svg" alt="projects" width="50"/><a>
 
 we can use our understandings of the world to develop intelligent systems 
that can interactive with the dynamic environments as we, humans, do. 



I am interested in combining the geometric 
understandings with the semanic interpretations of a scene as the first step towards this goal. 
 





---
Previously, I worked in image registration (aka. Optical Flow) and three-dimensional perception computer vision and 
how human intelligence can efficiently learn via interaction with the
environments as well as ho
interaction and intelligent systems that what we call 'intelligence' is, particularly in the domain of 
vision, perception and knowledge representation.  One way to study it is via reverse-engineer 
artificial systems that can computer vision and three-dimensional perception.

Understanding of the functional and causal relations between objects in a visual scene
Holistic scene interpretation by combining the semantic and geometric knowledge about 
2D images and 3D data (such as RGB-D) 

Recognition: What makes us recognize an object 
as what it is (e.g. a bird as a bird, a bull as a bull)? What is the necessary and 
sufficient representation of an object for human recognition? This question has been 
lingering on my mind ever since I saw a video of Picasso at work:

---
  I'm a machine learning researcher studying how intelligence can be 
computationally modeled and used to solve challenging social and 
environmental problems.  
 
the potential synergy between symbolic AI and deep learning 
incorporating knowledge and reasoning-based artificial intelligence to current 
deep learning approaches. 
reasoning.  I'm interested in bridging the how human intelligence can be
computationally modelled and MIT EECS (concentration: AI) pursuing a Masters in Engineering under the joint supervision of Professor Regina Barzilay, Professor Wojciech Matusik, and a Ph.D candidate, Julian Straub. Before my Masters, I studied Mathematics and EECS at MIT for my undergraduate studies.

I'm curious about what we call 'intelligence', especially in the domain of computer vision and three-dimensional perception.

Understanding of the functional and causal relations between objects in a visual scene
Holistic scene interpretation by combining the semantical and geometric information from 2D images and 3D data (such as RGB-D)
Recognition: What makes us recognize an object as what it is (e.g. a bird as a bird, a bull as a bull)? What is	the necessary and sufficient representation of an object for human recognition? This question has been lingering on my mind ever since I saw a video of Picasso at work:

-->