Title: About me

{% img ../images/profile.jpg %}


Hello!  I'm a PhD student at **University of Southern California** (USC) 
in Computer Science, studying:

(1) the **interpretability** and **transferability** of deep neural networks, and

(2) developing new intelligent systems that incorporate knowledge-based 
(symbolic) AI to neural network models, particularly with **spatio-temporal** data

At USC, I'm working with [Professor Craig Knoblock](http://usc-isi-i2.github.io/knoblock/) 
([Center on Knowledge Graphs](http://usc-isi-i2.github.io/home/) at ISI) and 
[Professor Yao-Yi Chiang](https://yaoyichi.github.io/) ([Spatial Computing and Informatics](http://spatial-computing.github.io/))
on the following projects.  Please see my [project](#) page for more details.

- Semantic Road
- MINT 


Before USC, I studied Mathematics and Electrical Engineering and Computer Science (EECS) 
 at **Massachusetts Institute of Technology** for my undergraduate studies, and 
 continued for Masters in Engineering in EECS with a concentration on AI. 
During my Masters, I worked under the joint guidance of [Professor Regina Barzilay](#), 
[Professor Wojciech Matusik](#), and [Dr. Julian Straub](#). 

- 
- 
- 

{% youtube ow7eEWh37iU %}

## Lingering questions

In a bigger scheme, I am excited about the problems regarding human perception of 
the world and how symbolic representation of knowledge can facilitate learning 
in a new domain [via knowledge transfer across various domains/modalities].  

1. How can intelligent agents learn with less supervision, particularly in the domain of 
vision and three-dimensional perception (<todo>:spatial reasonging?)
    - via autonomously interacting with the environment
    - via incorporating external knowledge 
    - via incorporating common sense reasoning
    
    My current [project](#semantic_road_project) on road detection from satellite images explores 
this question using external geospatial knowledge base (OpenStreetMap) and reinforcement
learning. <todo> transfer learning? 

2. How can those knowledge be represented in a more abstract form so that it can be used for 
learning in different domains 
(keyword: [Knowledge Representation](#), [Transfer Learning](#)/[Domain Adaptation](#))



##  Softwares
 
My preferred languages are Python (eg. **PyTorch**, Numpy, scikit-learn) for machine learning 
[projects](#), and C++ for hardware systems (eg. Microsoft Kinnect and Intel RealSense) 
as in [this](#) project. 

---
Besides working on my projects, I enjoy being in nature and trying out new 
activities outdoor. Along the way, I became a certified scuba diver, and sky-dived 
in Czech sky! I enjoy biking and swimming -- they help me connect to the part that 
is not about our minds, and remind me we are more than our 
thoughts and knowledge.  I enjoy sharing such experiences with friends:)
 
 
 
 ---
 

<!--
I hope everyday I become more aware of 

 
 we can use our understandings of the world to develop intelligent systems 
that can interactive with the dynamic environments as we, humans, do. 



I am interested in combining the geometric 
understandings with the semanic interpretations of a scene as the first step towards this goal. 
 





---
Previously, I worked in image registration (aka. Optical Flow) and three-dimensional perception computer vision and 
how human intelligence can efficiently learn via interaction with the
environments as well as ho
interaction and intelligent systems that what we call 'intelligence' is, particularly in the domain of 
vision, perception and knowledge representation.  One way to study it is via reverse-engineer 
artificial systems that can computer vision and three-dimensional perception.

Understanding of the functional and causal relations between objects in a visual scene
Holistic scene interpretation by combining the semantic and geometric knowledge about 
2D images and 3D data (such as RGB-D) 

Recognition: What makes us recognize an object 
as what it is (e.g. a bird as a bird, a bull as a bull)? What is the necessary and 
sufficient representation of an object for human recognition? This question has been 
lingering on my mind ever since I saw a video of Picasso at work:

---
  I'm a machine learning researcher studying how intelligence can be 
computationally modeled and used to solve challenging social and 
environmental problems.  
 
the potential synergy between symbolic AI and deep learning 
incorporating knowledge and reasoning-based artificial intelligence to current 
deep learning approaches. 
reasoning.  I'm interested in bridging the how human intelligence can be
computationally modelled and MIT EECS (concentration: AI) pursuing a Masters in Engineering under the joint supervision of Professor Regina Barzilay, Professor Wojciech Matusik, and a Ph.D candidate, Julian Straub. Before my Masters, I studied Mathematics and EECS at MIT for my undergraduate studies.

I'm curious about what we call 'intelligence', especially in the domain of computer vision and three-dimensional perception.

Understanding of the functional and causal relations between objects in a visual scene
Holistic scene interpretation by combining the semantical and geometric information from 2D images and 3D data (such as RGB-D)
Recognition: What makes us recognize an object as what it is (e.g. a bird as a bird, a bull as a bull)? What is	the necessary and sufficient representation of an object for human recognition? This question has been lingering on my mind ever since I saw a video of Picasso at work:

-->